{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the necessary Packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from sklearn.metrics import average_precision_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>Attribute1</th>\n",
       "      <th>Attribute2</th>\n",
       "      <th>Attribute3</th>\n",
       "      <th>Attribute4</th>\n",
       "      <th>Attribute5</th>\n",
       "      <th>Attribute6</th>\n",
       "      <th>Attribute7</th>\n",
       "      <th>Attribute8</th>\n",
       "      <th>Attribute9</th>\n",
       "      <th>...</th>\n",
       "      <th>Attribute21</th>\n",
       "      <th>Attribute22</th>\n",
       "      <th>Attribute23</th>\n",
       "      <th>Attribute24</th>\n",
       "      <th>Attribute25</th>\n",
       "      <th>Attribute26</th>\n",
       "      <th>Attribute27</th>\n",
       "      <th>Attribute28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>1.759072e-12</td>\n",
       "      <td>-8.251146e-13</td>\n",
       "      <td>-9.655448e-13</td>\n",
       "      <td>8.321385e-13</td>\n",
       "      <td>1.649983e-13</td>\n",
       "      <td>4.248434e-13</td>\n",
       "      <td>-3.054696e-13</td>\n",
       "      <td>8.777981e-14</td>\n",
       "      <td>-1.179757e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.405785e-13</td>\n",
       "      <td>-5.723165e-13</td>\n",
       "      <td>-9.725860e-13</td>\n",
       "      <td>1.464148e-12</td>\n",
       "      <td>-6.987110e-13</td>\n",
       "      <td>-5.617884e-13</td>\n",
       "      <td>3.332082e-12</td>\n",
       "      <td>-3.518875e-12</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time    Attribute1    Attribute2    Attribute3    Attribute4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  1.759072e-12 -8.251146e-13 -9.655448e-13  8.321385e-13   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "         Attribute5    Attribute6    Attribute7    Attribute8    Attribute9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   1.649983e-13  4.248434e-13 -3.054696e-13  8.777981e-14 -1.179757e-12   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...   Attribute21   Attribute22   Attribute23   Attribute24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ... -3.405785e-13 -5.723165e-13 -9.725860e-13  1.464148e-12   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "        Attribute25   Attribute26   Attribute27   Attribute28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean  -6.987110e-13 -5.617884e-13  3.332082e-12 -3.518875e-12      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_credit_card_history = pd.read_csv(\"data/creditcard.csv\")\n",
    "data_credit_card_history.head()\n",
    "data_credit_card_history.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"'Time' feature has an extremely large standard deviation, and viewing some of the data_credit_card_historyset above, it seems to be different for almost every data_credit_card_history instance. Since training a machine learning model on this feature will likely lead to overfitting it,  we shall drop 'Time' from our data_credit_card_historyset.\"\"\"\n",
    "\n",
    "data_credit_card_history.drop(\"Time\", axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd7f09eda20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"There is apparently a huge class imbalance in this data_credit_card_historyset. We can confirm this by plotting a seaborn countplot of the different class labels.\"\"\"\n",
    "\n",
    "sns.countplot(data_credit_card_history[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        30\n",
       "unique        1\n",
       "top       False\n",
       "freq         30\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"As we can see from the above plot, fraudulent activities make up a very small fraction of this data_credit_card_historyset. Let's further check to see if there are any null values in the data_credit_card_history.\"\"\"\n",
    "\n",
    "data_credit_card_history.isnull().any().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "\n",
    "## Training a Model\n",
    "\n",
    "90% of the data_credit_card_history will be a part of the training set and 5% will be allocated for each of the validation and test set.\n",
    "\"\"\"\n",
    "\n",
    "train_limit = int(0.9*len(data_credit_card_history))\n",
    "train = data_credit_card_history.loc[:train_limit]\n",
    "test_validation = data_credit_card_history.loc[train_limit:]\n",
    "test_validation.reset_index(drop=True, inplace=True)\n",
    "test_validation_train_limit = int(0.5*len(test_validation))\n",
    "val = test_validation.loc[:test_validation_train_limit]\n",
    "test = test_validation.loc[test_validation_train_limit:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of fraudulent transactions in the validation set: 11\n",
      "Number of fraudulent transactions in the test set: 11\n"
     ]
    }
   ],
   "source": [
    "\"\"\"validation and test set include a fair amount of fraudulent activites before going any further.\"\"\"\n",
    "\n",
    "print(\"Number of fraudulent transactions in the validation set: {}\"\\\n",
    "      .format(val[\"Class\"].value_counts()[1]))\n",
    "print(\"Number of fraudulent transactions in the test set: {}\"\\\n",
    "      .format(test[\"Class\"].value_counts()[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Now we can focus on developing a model to accurately detect fraudulent activity. Due to the huge class imbalance in our data_credit_card_historyset, a model that simply identifies all transactions as not being fraudulent would score high accuracy. There also would not be many fraudulent samples for the model to learn from to be able to accurately identify what a fraulent transaction is. Therefore, we should find a way to balance out the number of positive and negatives instances in our training set. This can be done by either oversampling the positive instances, or undersampling the negative instances. Undersampling the negative instances would involve reducing the number of non-fraudulent transactions until the ratio between positive and negative instances was approximately 1-to-1. Since we don't have that many data_credit_card_history samples, I fear doing so would severely train_limit our model's performance, since it would have much less data_credit_card_history to train on. We shall therefore oversample the positive instances in our training data_credit_card_history.\n",
    "\n",
    "To oversample the positive instances in our training set, we will add copies of them to it, but with their feature values slightly tweaked. This slight manipulation of the data_credit_card_history is done so as to have the copies being different from their original counterparts, but not too different as to end up teaching our model false information. This tweaking will be done by multiplying each positive sample copy's feature values by a number between the uniform distribution of 0.9 and 1.1.\n",
    "\"\"\"\n",
    "\n",
    "train_positive = train[train[\"Class\"] == 1]\n",
    "train_positive = pd.concat([train_positive] * int(len(train) / len(train_positive)), ignore_index=True)\n",
    "noise = np.random.uniform(0.9, 1.1, train_positive.shape)\n",
    "train_positive = train_positive.multiply(noise)\n",
    "train_positive[\"Class\"] = 1\n",
    "train_extended = train.append(train_positive, ignore_index=True)\n",
    "train_shuffled = train_extended.sample(frac=1, random_state=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.6/site-packages/seaborn/_decorators.py:43: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  FutureWarning\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd7f08b2860>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZgAAAEGCAYAAABYV4NmAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAATKklEQVR4nO3dcaxe9X3f8fcnNmFsDcwGhxKbxbQ424CtpHgOarQpHZrtVtogFXQ3U4O1WXOFyNRUVSWotBGBLBUtKStZoSLDwaAuYEFTmBZGPciWVWPAJbJqDEN4IQsOHnZqi9JKsJp898fzu+Px9ePLg3N/9zHX75d09Jzne87v9/wOsvThd86556SqkCRpvn1g0gOQJC1OBowkqQsDRpLUhQEjSerCgJEkdbF00gM4WZxzzjm1evXqSQ9Dkt5Xnn322R9U1YpR2wyYZvXq1UxPT096GJL0vpLkfx9vm6fIJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEld+Jf88+iyX7930kPQSejZf33tpIcAwPdu/luTHoJOQn/tX+3u1rczGElSF90CJsn5Sb6Z5IUke5L8Sqt/Icn3k+xqy88Ptbkxyd4kLybZMFS/LMnutu32JGn105M80OpPJVk91GZTkpfasqnXcUqSRut5iuwI8GtV9e0kHwKeTbKzbbutqr44vHOSi4Ap4GLgI8B/TvKxqnobuBPYAvwP4BvARuBRYDNwuKouTDIF3Ar84yTLgZuAtUC1336kqg53PF5J0pBuM5iq2l9V327rbwAvACvnaHIlcH9VvVVVLwN7gXVJzgPOrKonq6qAe4Grhtpsb+sPAle02c0GYGdVHWqhspNBKEmSFsiCXINpp64+DjzVSp9L8sdJtiVZ1morgVeGmu1rtZVtfXb9qDZVdQR4HTh7jr5mj2tLkukk0wcPHjzh45MkHat7wCT5MeAh4PNV9acMTnf9JHApsB/40syuI5rXHPUTbfNOoequqlpbVWtXrBj5vhxJ0gnqGjBJTmMQLr9XVb8PUFWvVdXbVfVD4CvAurb7PuD8oeargFdbfdWI+lFtkiwFzgIOzdGXJGmB9LyLLMDdwAtV9VtD9fOGdvs08FxbfwSYaneGXQCsAZ6uqv3AG0kub31eCzw81GbmDrGrgSfadZrHgPVJlrVTcOtbTZK0QHreRfZJ4LPA7iS7Wu03gM8kuZTBKavvAr8MUFV7kuwAnmdwB9r17Q4ygOuAe4AzGNw99mir3w3cl2Qvg5nLVOvrUJJbgGfafjdX1aFOxylJGqFbwFTVHzH6Wsg35mizFdg6oj4NXDKi/iZwzXH62gZsG3e8kqT55V/yS5K6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIXBowkqQsDRpLUhQEjSeqiW8AkOT/JN5O8kGRPkl9p9eVJdiZ5qX0uG2pzY5K9SV5MsmGoflmS3W3b7UnS6qcneaDVn0qyeqjNpvYbLyXZ1Os4JUmj9ZzBHAF+rar+JnA5cH2Si4AbgMerag3wePtO2zYFXAxsBO5IsqT1dSewBVjTlo2tvhk4XFUXArcBt7a+lgM3AZ8A1gE3DQeZJKm/bgFTVfur6ttt/Q3gBWAlcCWwve22HbiqrV8J3F9Vb1XVy8BeYF2S84Azq+rJqirg3lltZvp6ELiizW42ADur6lBVHQZ28k4oSZIWwIJcg2mnrj4OPAWcW1X7YRBCwIfbbiuBV4aa7Wu1lW19dv2oNlV1BHgdOHuOvmaPa0uS6STTBw8ePPEDlCQdo3vAJPkx4CHg81X1p3PtOqJWc9RPtM07haq7qmptVa1dsWLFHEOTJL1XXQMmyWkMwuX3qur3W/m1dtqL9nmg1fcB5w81XwW82uqrRtSPapNkKXAWcGiOviRJC6TnXWQB7gZeqKrfGtr0CDBzV9cm4OGh+lS7M+wCBhfzn26n0d5Icnnr89pZbWb6uhp4ol2neQxYn2RZu7i/vtUkSQtkace+Pwl8FtidZFer/Qbwm8COJJuB7wHXAFTVniQ7gOcZ3IF2fVW93dpdB9wDnAE82hYYBNh9SfYymLlMtb4OJbkFeKbtd3NVHep1oJKkY3ULmKr6I0ZfCwG44jhttgJbR9SngUtG1N+kBdSIbduAbeOOV5I0v/xLfklSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC66BUySbUkOJHluqPaFJN9PsqstPz+07cYke5O8mGTDUP2yJLvbttuTpNVPT/JAqz+VZPVQm01JXmrLpl7HKEk6vp4zmHuAjSPqt1XVpW35BkCSi4Ap4OLW5o4kS9r+dwJbgDVtmelzM3C4qi4EbgNubX0tB24CPgGsA25Ksmz+D0+SNJduAVNV3wIOjbn7lcD9VfVWVb0M7AXWJTkPOLOqnqyqAu4Frhpqs72tPwhc0WY3G4CdVXWoqg4DOxkddJKkjiZxDeZzSf64nUKbmVmsBF4Z2mdfq61s67PrR7WpqiPA68DZc/R1jCRbkkwnmT548OCPdlSSpKOMFTBJHh+nNoY7gZ8ELgX2A1+a6W7EvjVH/UTbHF2suquq1lbV2hUrVsw1bknSezRnwCT5S+2axjlJliVZ3pbVwEfe649V1WtV9XZV/RD4CoNrJDCYZZw/tOsq4NVWXzWiflSbJEuBsxickjteX5KkBfRuM5hfBp4F/kb7nFkeBn7nvf5Yu6Yy49PAzB1mjwBT7c6wCxhczH+6qvYDbyS5vF1fubb99kybmTvErgaeaNdpHgPWt0BcBqxvNUnSAlo618aq+m3gt5P8i6r68nvpOMnXgE8xmP3sY3Bn16eSXMrglNV3GQQYVbUnyQ7geeAIcH1Vvd26uo7BHWlnAI+2BeBu4L4kexnMXKZaX4eS3AI80/a7uarGvdlAkjRP5gyYGVX15SQ/A6weblNV987R5jMjynfPsf9WYOuI+jRwyYj6m8A1x+lrG7DteL8lSepvrIBJch+Di/O7gJmZxcxtw5IkHWOsgAHWAhe1axySJL2rcf8O5jngx3sORJK0uIw7gzkHeD7J08BbM8Wq+kddRiVJet8bN2C+0HMQkqTFZ9y7yP5r74FIkhaXce8ie4N3HrfyQeA04M+r6sxeA5Mkvb+NO4P50PD3JFfxzmNeJEk6xgk9Tbmq/gD4+/M8FknSIjLuKbJfGPr6AQZ/F+PfxEiSjmvcu8j+4dD6EQbPEbty3kcjSVo0xr0G8097D0SStLiM+8KxVUm+nuRAkteSPJRk1bu3lCSdqsa9yP9VBu9f+QiD1w//h1aTJGmkcQNmRVV9taqOtOUewHcMS5KOa9yA+UGSX0qypC2/BPxJz4FJkt7fxg2Yfwb8IvB/gP0MXlHshX9J0nGNe5vyLcCmqjoMkGQ58EUGwSNJ0jHGncH87ZlwgcF774GP9xmSJGkxGDdgPpBk2cyXNoMZd/YjSToFjRsSXwL+e5IHGTwi5heBrd1GJUl63xv3L/nvTTLN4AGXAX6hqp7vOjJJ0vva2Ke5WqAYKpKksZzQ4/olSXo3BowkqQsDRpLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLroFTJJt7RXLzw3VlifZmeSl9jn8fLMbk+xN8mKSDUP1y5LsbttuT5JWPz3JA63+VJLVQ202td94KcmmXscoSTq+njOYe4CNs2o3AI9X1Rrg8fadJBcBU8DFrc0dSZa0NncCW4A1bZnpczNwuKouBG4Dbm19LQduAj4BrANuGg4ySdLC6BYwVfUt4NCs8pXA9ra+HbhqqH5/Vb1VVS8De4F1Sc4DzqyqJ6uqgHtntZnp60Hgija72QDsrKpD7RUDOzk26CRJnS30NZhzq2o/QPv8cKuvBF4Z2m9fq61s67PrR7WpqiPA68DZc/R1jCRbkkwnmT548OCPcFiSpNlOlov8GVGrOeon2uboYtVdVbW2qtauWLFirIFKksaz0AHzWjvtRfs80Or7gPOH9lsFvNrqq0bUj2qTZClwFoNTcsfrS5K0gBY6YB4BZu7q2gQ8PFSfaneGXcDgYv7T7TTaG0kub9dXrp3VZqavq4En2nWax4D1SZa1i/vrW02StIC6vfY4ydeATwHnJNnH4M6u3wR2JNkMfA+4BqCq9iTZweB9M0eA66vq7dbVdQzuSDsDeLQtAHcD9yXZy2DmMtX6OpTkFuCZtt/NVTX7ZgNJUmfdAqaqPnOcTVccZ/+tjHgNc1VNA5eMqL9JC6gR27YB28YerCRp3p0sF/klSYuMASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjCSpCwNGktSFASNJ6sKAkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcTCZgk302yO8muJNOttjzJziQvtc9lQ/vfmGRvkheTbBiqX9b62Zvk9iRp9dOTPNDqTyVZvdDHKEmnuknOYH62qi6tqrXt+w3A41W1Bni8fSfJRcAUcDGwEbgjyZLW5k5gC7CmLRtbfTNwuKouBG4Dbl2A45EkDTmZTpFdCWxv69uBq4bq91fVW1X1MrAXWJfkPODMqnqyqgq4d1abmb4eBK6Ymd1IkhbGpAKmgD9M8mySLa12blXtB2ifH271lcArQ233tdrKtj67flSbqjoCvA6cPXsQSbYkmU4yffDgwXk5MEnSwNIJ/e4nq+rVJB8Gdib5n3PsO2rmUXPU52pzdKHqLuAugLVr1x6zXZJ04iYyg6mqV9vnAeDrwDrgtXbai/Z5oO2+Dzh/qPkq4NVWXzWiflSbJEuBs4BDPY5FkjTaggdMkr+S5EMz68B64DngEWBT220T8HBbfwSYaneGXcDgYv7T7TTaG0kub9dXrp3VZqavq4En2nUaSdICmcQpsnOBr7dr7kuBf19V/ynJM8COJJuB7wHXAFTVniQ7gOeBI8D1VfV26+s64B7gDODRtgDcDdyXZC+DmcvUQhyYJOkdCx4wVfUd4KdG1P8EuOI4bbYCW0fUp4FLRtTfpAWUJGkyTqbblCVJi4gBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKkLA0aS1IUBI0nqwoCRJHVhwEiSujBgJEldGDCSpC4MGElSFwaMJKmLRR0wSTYmeTHJ3iQ3THo8knQqWbQBk2QJ8DvAzwEXAZ9JctFkRyVJp45FGzDAOmBvVX2nqv4vcD9w5YTHJEmnjKWTHkBHK4FXhr7vAz4xvEOSLcCW9vXPkry4QGM7FZwD/GDSgzgZ5IubJj0EHct/nzNuyo/aw0ePt2ExB8yo/2p11Jequ4C7FmY4p5Yk01W1dtLjkEbx3+fCWMynyPYB5w99XwW8OqGxSNIpZzEHzDPAmiQXJPkgMAU8MuExSdIpY9GeIquqI0k+BzwGLAG2VdWeCQ/rVOKpR53M/Pe5AFJV776XJEnv0WI+RSZJmiADRpLUhQGjeecjenQySrItyYEkz016LKcKA0bzykf06CR2D7Bx0oM4lRgwmm8+okcnpar6FnBo0uM4lRgwmm+jHtGzckJjkTRBBozm27s+okfSqcGA0XzzET2SAANG889H9EgCDBjNs6o6Asw8oucFYIeP6NHJIMnXgCeBv55kX5LNkx7TYuejYiRJXTiDkSR1YcBIkrowYCRJXRgwkqQuDBhJUhcGjDQBSX48yf1J/leS55N8I8nHfNKvFpNF+8pk6WSVJMDXge1VNdVqlwLnTnRg0jxzBiMtvJ8F/qKqfnemUFW7GHpIaJLVSf5bkm+35Wda/bwk30qyK8lzSf5ukiVJ7mnfdyf51YU/JOlYzmCkhXcJ8Oy77HMA+AdV9WaSNcDXgLXAPwEeq6qt7d07fxm4FFhZVZcAJPmr/YYujc+AkU5OpwH/tp06exv4WKs/A2xLchrwB1W1K8l3gJ9I8mXgPwJ/OJERS7N4ikxaeHuAy95ln18FXgN+isHM5YPw/1+a9feA7wP3Jbm2qg63/f4LcD3w7/oMW3pvDBhp4T0BnJ7kn88Ukvwd4KND+5wF7K+qHwKfBZa0/T4KHKiqrwB3Az+d5BzgA1X1EPAvgZ9emMOQ5uYpMmmBVVUl+TTwb5LcALwJfBf4/NBudwAPJbkG+Cbw563+KeDXk/wF8GfAtQzeGPrVJDP/w3hj94OQxuDTlCVJXXiKTJLUhQEjSerCgJEkdWHASJK6MGAkSV0YMJKkLgwYSVIX/w+u5FOWFM0LKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"The ratio of positive to negative instances in our training set should now be much more balanced.\"\"\"\n",
    "\n",
    "sns.countplot(train_shuffled[\"Class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Spliting train, validation, and test data_credit_card_history into their predictors and labels.\"\"\"\n",
    "\n",
    "X_train = train_shuffled.drop(labels=[\"Class\"], axis=1)\n",
    "Y_train = train_shuffled[\"Class\"]\n",
    "X_val = val.drop(labels=[\"Class\"], axis=1)\n",
    "Y_val = val[\"Class\"]\n",
    "X_test = test.drop(labels=[\"Class\"], axis=1)\n",
    "Y_test = test[\"Class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"standardize the values in our data_credit_card_historyset so that when building a machine learning model we don't unintentionally lend more weight to some features over others. We will fit a standardizer to the training data_credit_card_history, and transform the training, validation, and test data_credit_card_history based on this scaler.\"\"\"\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train)\n",
    "X_val[X_val.columns] = scaler.transform(X_val)\n",
    "X_test[X_test.columns] = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 64)                1920      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 2)                 10        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 3         \n",
      "=================================================================\n",
      "Total params: 4,713\n",
      "Trainable params: 4,713\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "16015/16015 [==============================] - 58s 4ms/step - loss: 0.3585 - accuracy: 0.9192 - val_loss: 0.2249 - val_accuracy: 0.9957\n",
      "Epoch 2/50\n",
      "16015/16015 [==============================] - 56s 4ms/step - loss: 0.1611 - accuracy: 0.9614 - val_loss: 0.1011 - val_accuracy: 0.9969\n",
      "Epoch 3/50\n",
      " 7373/16015 [============>.................] - ETA: 29s - loss: 0.1385 - accuracy: 0.9626"
     ]
    }
   ],
   "source": [
    "\"\"\"Model build and train a feedforward neural network to detect fraudulent activity. The neural network will contain several layers of hidden units with ReLU activation functions, and a sigmoid output unit to output the probability of a given transaction being fraudulent. The Adam optimizer will be used with an initial learning rate of 1e-4 and a binary cross-entropy loss function. The model will be trained for a maximum of 50 epochs, though the learning rate of the Adam optimizer will be reduced by a factor of 0.1 if the validation loss does not improve after 3 epochs of training. This will continue until a minimum learning rate of 1e-6 is reached. If the validation loss does not improve after 5 epochs of training, we will halt training of the neural network.\"\"\"\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation=\"relu\", input_dim=(X_train.shape[1])))\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dense(16, activation=\"relu\"))\n",
    "model.add(Dense(8, activation=\"relu\"))\n",
    "model.add(Dense(4, activation=\"relu\"))\n",
    "model.add(Dense(2, activation=\"relu\"))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(optimizer=Adam(lr=1e-4), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "history = model.fit(X_train, \n",
    "                    Y_train, \n",
    "                    epochs=50, \n",
    "                    validation_data=(X_val, Y_val), \n",
    "                    callbacks=[ReduceLROnPlateau(patience=3, verbose=1, min_lr=1e-6), \n",
    "                               EarlyStopping(patience=5, verbose=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"## Analyzing our Model\n",
    "\n",
    "With training ceased for our neural network, let's observe how the loss and accuracy evolved during training for both the training and validation set.\n",
    "\"\"\"\n",
    "\n",
    "number_of_epochs = len(history.history[\"loss\"])\n",
    "fig, axarr = plt.subplots(1, 2, figsize=(24, 8))\n",
    "axarr[0].set_xlabel(\"Number of Epochs\")\n",
    "axarr[0].set_ylabel(\"Loss\")\n",
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"loss\"], label=\"Train\", ax=axarr[0])\n",
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"val_loss\"], label=\"Validation\", ax=axarr[0])\n",
    "axarr[1].set_xlabel(\"Number of Epochs\")\n",
    "axarr[1].set_ylabel(\"Accuracy\")\n",
    "axarr[1].set_ylim(0, 1)\n",
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"accuracy\"], label=\"Train\", ax=axarr[1])\n",
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"val_accuracy\"], label=\"Validation\", ax=axarr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"Viewing the loss and accuracy graphs, we can see that after some initial improvements, we seemed to reach convergence after only a few epochs. Let's view the accuracy achieved by this neural network on the test set.\"\"\"\n",
    "\n",
    "test_results = model.evaluate(X_test, Y_test)\n",
    "print(\"The model test accuracy is {}.\".format(test_results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"accuracy\"], label=\"Train\", ax=axarr[1])\n",
    "sns.lineplot(x=range(1, number_of_epochs+1), y=history.history[\"val_accuracy\"], label=\"Validation\", ax=axarr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Achieving almost 100% accuracy on the test set is of course thrilling, but one must not forget about the huge class imbalance still present in the test set. A model that simply outputted that there were no fraudulent transactions would achieve high accuracy as well. The data_credit_card_historyset info recommends using the AUPRC as an evaluation metric instead, to gather a better understanding of how well the model truly performed. We will use the average precision score instead, which is an evaluation metric available in scikit-learn that is sometimes used as an alternative to AUPRC. The average precision score will have a value between 0 and 1, with a better model having a greater score. Let's view the test average precision score for our model.\"\"\"\n",
    "\n",
    "model_prediction = model.predict_classes(X_test)\n",
    "ap_score = average_precision_score(Y_test, model_prediction)\n",
    "print(\"The model test average precision score is {}.\".format(ap_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\confusion_matrix matrix of the model_prediction made by the model on the test set, to gain a better idea of what type of errors it is making.\"\"\"\n",
    "confusion_matrix = pd.DataFrame(confusion_matrix(Y_test, model_prediction))\n",
    "confusion_matrix.columns = [\"Predicted Negative\", \"Predicted Positive\"]\n",
    "confusion_matrix.index = [\"Actual Negative\", \"Actual Positive\"]\n",
    "sns.heatmap(confusion_matrix, annot=True)\n",
    "plt.yticks(rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.sklearn.save_model(model, path='./credit_card_score_prediction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
